{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11bc2da2",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a5a475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ed76c",
   "metadata": {},
   "source": [
    "Importing MNIST dataset from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10bff1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fb4b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad46922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667ad913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba860daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b46e2",
   "metadata": {},
   "source": [
    "Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e460c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test = x_train/255.0,x_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f6fba1",
   "metadata": {},
   "source": [
    "Basic Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f4e17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(50, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f033ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 2ms/step - loss: 0.2595 - accuracy: 0.9234\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1111 - accuracy: 0.9669\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0779 - accuracy: 0.9767\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0586 - accuracy: 0.9815\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0476 - accuracy: 0.9845\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0754 - accuracy: 0.9763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0754140093922615, 0.9763000011444092]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1a2c41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0754 - accuracy: 0.9763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0754140093922615, 0.9763000011444092]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = model.evaluate(x_test,y_test)\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b87e2d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9763000011444092"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b52088bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "471d17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ReLu activation function with 2 hidden layers with 100 neurons in 1st layer and 50 in 2nd\n",
    "model_2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(50, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_2)\n",
    "## ReLu activation function with 2 hidden layers with 50 neurons in 1st layer and 100 in 2nd\n",
    "model_3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(50, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_3)\n",
    "## ReLu activation function with 2 hidden layers with 75 neurons in 1st layer and 75 in 2nd\n",
    "model_4 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(75, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(75, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_4)\n",
    "## Tanh activation function with 2 hidden layers with 100 neurons in 1st layer and 50 in 2nd\n",
    "model_5 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(50, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_5)\n",
    "## Tanh activation function with 2 hidden layers with 50 neurons in 1st layer and 100 in 2nd\n",
    "model_6 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(50, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(100, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_6)\n",
    "## Tanh activation function with 2 hidden layers with 75 neurons in 1st layer and 75 in 2nd\n",
    "model_7 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(75, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(75, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "\n",
    "models.append(model_7)\n",
    "## Sigmoid activation function with 2 hidden layers with 100 neurons in 1st layer and 50 in 2nd\n",
    "model_8 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(50, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_8)\n",
    "## Sigmoid activation function with 2 hidden layers with 50 neurons in 1st layer and 100 in 2nd\n",
    "model_9 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(50, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(100, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_9)\n",
    "## Sigmoid activation function with 2 hidden layers with 75 neurons in 1st layer and 75 in 2nd\n",
    "model_10 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(75, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(75, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e3f0ba",
   "metadata": {},
   "source": [
    "## 3 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afc54918",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sigmoid activation function with 3 hidden layers with 100 neurons in 1st layer and 25 in 2nd and 25 in 3rd layer\n",
    "model_11 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(25, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(25, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_11)\n",
    "## Sigmoid activation function with 3 hidden layers with 100 neurons in 1st layer and 30 in 2nd and 20 in 3rd layer\n",
    "model_12 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(30, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(20, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_12)\n",
    "## Sigmoid activation function with 3 hidden layers with 20 neurons in 1st layer and 30 in 2nd and 100 in 3rd layer\n",
    "model_13 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(20, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(30, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(100, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_13)\n",
    "## Sigmoid activation function with 3 hidden layers with 20 neurons in 1st layer and 30 in 2nd and 50 in 3rd layer\n",
    "model_14 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(20, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(30, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(50, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_14)\n",
    "## Sigmoid activation function with 3 hidden layers with 50 neurons in 1st layer and 30 in 2nd and 20 in 3rd layer\n",
    "model_15 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(50, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(30, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(20, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_15)\n",
    "## Sigmoid activation function with 3 hidden layers with 50 neurons in 1st layer and 20 in 2nd and 30 in 3rd layer\n",
    "model_16 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(50, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(20, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(30, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "676503b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tanh activation function with 3 hidden layers with 100 neurons in 1st layer and 25 in 2nd and 25 in 3rd layer\n",
    "model_17 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(25, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(25, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_17)\n",
    "## Tanh activation function with 3 hidden layers with 100 neurons in 1st layer and 30 in 2nd and 20 in 3rd layer\n",
    "model_18 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(30, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(20, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_18)\n",
    "## Tanh activation function with 3 hidden layers with 20 neurons in 1st layer and 30 in 2nd and 100 in 3rd layer\n",
    "model_19 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(20, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(30, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(100, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_19)\n",
    "## Tanh activation function with 3 hidden layers with 20 neurons in 1st layer and 30 in 2nd and 50 in 3rd layer\n",
    "model_20 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(20, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(30, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(50, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_20)\n",
    "## Tanh activation function with 3 hidden layers with 50 neurons in 1st layer and 30 in 2nd and 20 in 3rd layer\n",
    "model_21 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(50, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(30, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(20, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_21)\n",
    "## Tanh activation function with 3 hidden layers with 50 neurons in 1st layer and 20 in 2nd and 30 in 3rd layer\n",
    "model_22 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(50, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(20, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(30, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e29c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ReLu activation function with 3 hidden layers with 100 neurons in 1st layer and 25 in 2nd and 25 in 3rd layer\n",
    "model_23 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(25, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(25, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_23)\n",
    "## ReLu activation function with 3 hidden layers with 100 neurons in 1st layer and 30 in 2nd and 20 in 3rd layer\n",
    "model_24 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(30, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(20, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_24)\n",
    "## ReLu activation function with 3 hidden layers with 20 neurons in 1st layer and 30 in 2nd and 100 in 3rd layer\n",
    "model_25 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(20, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(30, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_25)\n",
    "## ReLu activation function with 3 hidden layers with 20 neurons in 1st layer and 30 in 2nd and 50 in 3rd layer\n",
    "model_26 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(20, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(30, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(50, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_26)\n",
    "## ReLu activation function with 3 hidden layers with 50 neurons in 1st layer and 30 in 2nd and 20 in 3rd layer\n",
    "model_27 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(50, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(30, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(20, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_27)\n",
    "## ReLu activation function with 3 hidden layers with 50 neurons in 1st layer and 20 in 2nd and 30 in 3rd layer\n",
    "model_28 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(50, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(20, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(30, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b85ba",
   "metadata": {},
   "source": [
    "## Mixing activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "292a1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_29 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(30, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(20, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_29)\n",
    "model_30 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(50, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(30, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(20, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(10 , activation = 'softmax')\n",
    "])\n",
    "models.append(model_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf2cac8",
   "metadata": {},
   "source": [
    "## Evaluating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c32a26e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0403 - accuracy: 0.9868\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0349 - accuracy: 0.9887\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0271 - accuracy: 0.9910\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0240 - accuracy: 0.9919\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0202 - accuracy: 0.9932\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0943 - accuracy: 0.9773\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2507 - accuracy: 0.9261\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1080 - accuracy: 0.9674\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0767 - accuracy: 0.9758\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0577 - accuracy: 0.9818\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0470 - accuracy: 0.9845\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0881 - accuracy: 0.9741\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2813 - accuracy: 0.9162\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1311 - accuracy: 0.9604\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0973 - accuracy: 0.9693\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0774 - accuracy: 0.9762\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0632 - accuracy: 0.9801\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0963 - accuracy: 0.9724\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2689 - accuracy: 0.9216\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1160 - accuracy: 0.9650\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0828 - accuracy: 0.9741\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0651 - accuracy: 0.9789\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0511 - accuracy: 0.9829\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0962 - accuracy: 0.9709\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2714 - accuracy: 0.9228\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1239 - accuracy: 0.9626\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0863 - accuracy: 0.9740\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0648 - accuracy: 0.9800\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0501 - accuracy: 0.9844\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0961 - accuracy: 0.9691\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2836 - accuracy: 0.9182\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1349 - accuracy: 0.9595\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0982 - accuracy: 0.9701\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0779 - accuracy: 0.9755\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0649 - accuracy: 0.9799\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0960 - accuracy: 0.9706\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2723 - accuracy: 0.9203\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1248 - accuracy: 0.9625\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0866 - accuracy: 0.9731\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0653 - accuracy: 0.9795\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0515 - accuracy: 0.9836\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0856 - accuracy: 0.9737\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.5130 - accuracy: 0.8735\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1883 - accuracy: 0.9456\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1319 - accuracy: 0.9612\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0998 - accuracy: 0.9710\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0793 - accuracy: 0.9770\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0957 - accuracy: 0.9709\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.5317 - accuracy: 0.8629\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2047 - accuracy: 0.9402\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1544 - accuracy: 0.9547\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1240 - accuracy: 0.9638\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1030 - accuracy: 0.9698\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1127 - accuracy: 0.9672\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.5152 - accuracy: 0.8711\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1896 - accuracy: 0.9443\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1365 - accuracy: 0.9597\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1043 - accuracy: 0.9696\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0839 - accuracy: 0.9750\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0930 - accuracy: 0.9708\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 2ms/step - loss: 0.8256 - accuracy: 0.8187\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2290 - accuracy: 0.9401\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1526 - accuracy: 0.9581\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1145 - accuracy: 0.9689\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0909 - accuracy: 0.9746\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1090 - accuracy: 0.9679\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 2ms/step - loss: 0.8099 - accuracy: 0.8288\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2291 - accuracy: 0.9410\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1508 - accuracy: 0.9593\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1135 - accuracy: 0.9687\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0908 - accuracy: 0.9753\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1104 - accuracy: 0.9679\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.8622 - accuracy: 0.7596\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2875 - accuracy: 0.9182\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2274 - accuracy: 0.9344\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1941 - accuracy: 0.9439\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1747 - accuracy: 0.9487\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1821 - accuracy: 0.9472\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.9853 - accuracy: 0.7281\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3339 - accuracy: 0.9097\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2556 - accuracy: 0.9280\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2203 - accuracy: 0.9372\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1968 - accuracy: 0.9434\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2023 - accuracy: 0.9410\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.9215 - accuracy: 0.7743\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2793 - accuracy: 0.9288\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1892 - accuracy: 0.9488\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1480 - accuracy: 0.9600\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1233 - accuracy: 0.9666\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1348 - accuracy: 0.9599\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.9027 - accuracy: 0.7757\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2672 - accuracy: 0.9301\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1795 - accuracy: 0.9509\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1392 - accuracy: 0.9605\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1152 - accuracy: 0.9674\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1340 - accuracy: 0.9616\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 2ms/step - loss: 0.3085 - accuracy: 0.9193\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1289 - accuracy: 0.9616\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0898 - accuracy: 0.9730\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0721 - accuracy: 0.9778\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0547 - accuracy: 0.9832\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0984 - accuracy: 0.9697\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 2ms/step - loss: 0.3288 - accuracy: 0.9125\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1343 - accuracy: 0.9607\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0966 - accuracy: 0.9714\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0734 - accuracy: 0.9788\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0579 - accuracy: 0.9826\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0884 - accuracy: 0.9736\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3250 - accuracy: 0.9052\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1753 - accuracy: 0.9480\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1415 - accuracy: 0.9567\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1237 - accuracy: 0.9619\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1116 - accuracy: 0.9652\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1480 - accuracy: 0.9562\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3588 - accuracy: 0.8989\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1949 - accuracy: 0.9425\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1566 - accuracy: 0.9527\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1353 - accuracy: 0.9600\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1205 - accuracy: 0.9639\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1411 - accuracy: 0.9587\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3465 - accuracy: 0.9079\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1450 - accuracy: 0.9568\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1050 - accuracy: 0.9685\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0854 - accuracy: 0.9743\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0704 - accuracy: 0.9786\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1083 - accuracy: 0.9691\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 2ms/step - loss: 0.3353 - accuracy: 0.9098\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1428 - accuracy: 0.9585\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1056 - accuracy: 0.9685\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0857 - accuracy: 0.9739\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0733 - accuracy: 0.9772\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1052 - accuracy: 0.9689\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 2ms/step - loss: 0.3025 - accuracy: 0.9107\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1260 - accuracy: 0.9621\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0924 - accuracy: 0.9714\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0694 - accuracy: 0.9789\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0582 - accuracy: 0.9823\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0859 - accuracy: 0.9745\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 2ms/step - loss: 0.2911 - accuracy: 0.9154\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1223 - accuracy: 0.9633\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0878 - accuracy: 0.9727\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0684 - accuracy: 0.9781\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0556 - accuracy: 0.9823\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0899 - accuracy: 0.9742\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3578 - accuracy: 0.8926\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1883 - accuracy: 0.9445\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1546 - accuracy: 0.9528\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1348 - accuracy: 0.9587\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1200 - accuracy: 0.9632\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1362 - accuracy: 0.9585\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3608 - accuracy: 0.8906\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1790 - accuracy: 0.9460\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1484 - accuracy: 0.9546\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1298 - accuracy: 0.9608\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1168 - accuracy: 0.9643\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1337 - accuracy: 0.9595\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3449 - accuracy: 0.8983\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1466 - accuracy: 0.9568\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1097 - accuracy: 0.9672\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0880 - accuracy: 0.9736\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0760 - accuracy: 0.9767\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0979 - accuracy: 0.9714\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3284 - accuracy: 0.9046\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1495 - accuracy: 0.9556\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1151 - accuracy: 0.9656\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0927 - accuracy: 0.9721\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0804 - accuracy: 0.9754\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1073 - accuracy: 0.9680\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 2ms/step - loss: 0.4681 - accuracy: 0.9116\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1337 - accuracy: 0.9648\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 976us/step - loss: 0.0896 - accuracy: 0.9759\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 941us/step - loss: 0.0687 - accuracy: 0.9808\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 970us/step - loss: 0.0571 - accuracy: 0.9835\n",
      "313/313 [==============================] - 0s 766us/step - loss: 0.0976 - accuracy: 0.9726\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 861us/step - loss: 0.4326 - accuracy: 0.8859\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 866us/step - loss: 0.1791 - accuracy: 0.9472\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 899us/step - loss: 0.1315 - accuracy: 0.9613\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 862us/step - loss: 0.1049 - accuracy: 0.9689\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 843us/step - loss: 0.0874 - accuracy: 0.9734\n",
      "313/313 [==============================] - 0s 734us/step - loss: 0.1104 - accuracy: 0.9661\n"
     ]
    }
   ],
   "source": [
    "list_models=[]\n",
    "for i in models:\n",
    "    i.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    i.fit(x_train, y_train, epochs=5)\n",
    "    temp = i.evaluate(x_test, y_test)\n",
    "    list_models.append(temp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ab4d2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model No:  1    97.72999882698059\n",
      "Model No:  2    97.40999937057495\n",
      "Model No:  3    97.24000096321106\n",
      "Model No:  4    97.08999991416931\n",
      "Model No:  5    96.90999984741211\n",
      "Model No:  6    97.06000089645386\n",
      "Model No:  7    97.36999869346619\n",
      "Model No:  8    97.08999991416931\n",
      "Model No:  9    96.7199981212616\n",
      "Model No:  10    97.079998254776\n",
      "Model No:  11    96.78999781608582\n",
      "Model No:  12    96.78999781608582\n",
      "Model No:  13    94.72000002861023\n",
      "Model No:  14    94.0999984741211\n",
      "Model No:  15    95.99000215530396\n",
      "Model No:  16    96.16000056266785\n",
      "Model No:  17    96.96999788284302\n",
      "Model No:  18    97.35999703407288\n",
      "Model No:  19    95.62000036239624\n",
      "Model No:  20    95.87000012397766\n",
      "Model No:  21    96.90999984741211\n",
      "Model No:  22    96.89000248908997\n",
      "Model No:  23    97.45000004768372\n",
      "Model No:  24    97.42000102996826\n",
      "Model No:  25    95.85000276565552\n",
      "Model No:  26    95.95000147819519\n",
      "Model No:  27    97.14000225067139\n",
      "Model No:  28    96.79999947547913\n",
      "Model No:  29    97.2599983215332\n",
      "Model No:  30    96.60999774932861\n"
     ]
    }
   ],
   "source": [
    "cou=1\n",
    "for i in list_models:\n",
    "    print('Model No:  '+str(cou)+'    '+ str(i*100))\n",
    "    cou=cou+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd9eb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b7aea4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 638us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(model.predict(x_test),axis=1)\n",
    "mat = confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38f433df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 955,    0,    1,    0,    1,    3,    1,    1,   16,    2],\n",
       "       [   0, 1104,    6,    1,    0,    1,    3,    2,   17,    1],\n",
       "       [   5,    0,  975,    7,    1,    1,    2,    1,   39,    1],\n",
       "       [   0,    0,    3,  958,    0,    1,    1,    4,   42,    1],\n",
       "       [   0,    0,    2,    1,  914,    0,    8,    2,   22,   33],\n",
       "       [   3,    0,    0,   15,    4,  829,    5,    0,   32,    4],\n",
       "       [   5,    2,    0,    0,    7,   35,  901,    0,    8,    0],\n",
       "       [   1,    3,   21,    7,    3,    4,    0,  956,   21,   12],\n",
       "       [   6,    0,    1,    3,    1,    9,    2,    0,  942,   10],\n",
       "       [   2,    3,    0,    6,    9,    2,    0,    4,   26,  957]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Confusion Matrix : \")\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e927e9",
   "metadata": {},
   "source": [
    "## Remark:\n",
    "\n",
    "As we can see training around 30 models by varying parameters all models perform more or less the same i.e there is no significant difference between them although the models with ReLu as their activation function perform slightly better"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
